---
layout: post
title: eyeNav
description: Eye-control interface for people with ALS
image: https://img.youtube.com/vi/z3LSZCZcINg/maxresdefault.jpg
#image: 
tags: hackathon ui
categories: interaction

---

EyeNav is an **Eye-control** interface for people with ALS.  
EyeNav is using live video stream from a camera located on glasses frame, pointed directly to the user's eye.
The video is processed and analyzed using computer vision algorithm and translates the eye movement to keystrokes (up/down/left/right)


<iframe width="560" height="315" src="https://www.youtube.com/embed/z3LSZCZcINg" frameborder="0" allowfullscreen></iframe>

inspired by the <a href="https://www.eyewriter.org/">eyeWriter</a> project, and uses same low-cost hardware (webcam on plastic glasses).  
What makes EyeNav different is that unlike direct, mouse-like cursor manipulation, which can be tiresome -
it is based on discrete gestures (like keyboard), for navigation.

The project was conceived, born, and took its first steps in 12 hours, during a Hackathon for apps made for disabled people

<a href="https://eranws.github.io/eyeNav/">project page
</a>

